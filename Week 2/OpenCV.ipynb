{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV\n",
    "**Prerequisite:** Before starting this exercise, you should make yourself familiar with Python and some necessary library, e.g., numpy, matplotlib, etc. One good tutorial can be found [here](http://cs231n.github.io/python-numpy-tutorial/).\n",
    "\n",
    "In this exercise you will:\n",
    "* Learn about some basic image processing operations with OpenCV.\n",
    "* Re-implement some basic image processing operations. This will help you to\n",
    " * Have better understand about the image processing operations.\n",
    " * Practice Python programming with Numpy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge opencv \n",
    "!conda install -c conda-forge tensorflow\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_error(out, correct_out):\n",
    "    return np.sum(abs(out.astype(np.float32) - correct_out.astype(np.float32)) / \n",
    "                          (abs(out.astype(np.float32)) + abs(correct_out.astype(np.float32))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking OpenCV version\n",
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTICE: \n",
    "In this lab exercise, we recommend to use `OpenCV 3.x` version, the documentations for OpenCV API can be found [here](https://docs.opencv.org/3.0-beta/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images\n",
    "Use the function [cv2.imread()](https://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/reading_and_writing_images.html?highlight=cv2.imread#cv2.imread) to read an image. The image should be in the working directory or a full path of image should be given. The function will return a numpy matrix.\n",
    "\n",
    "Second argument is a flag which specifies the way image should be read.\n",
    "\n",
    " * cv2.IMREAD_COLOR - (1): Loads a color image. Any transparency (alpha channel) of image will be neglected. It is the **default flag**.\n",
    " * cv2.IMREAD_GRAYSCALE - (0): Loads image in grayscale mode\n",
    " * cv2.IMREAD_UNCHANGED - (-1): Loads image as such including alpha channel, if included.\n",
    " \n",
    "**NOTE**: Color image loaded by OpenCV is in *Blue-Green-Red (BGR)* mode. But Matplotlib displays in *RGB* mode. So color images will not be displayed correctly in Matplotlib if image is read with OpenCV. We will discuss how to handle to display properly later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray = cv2.imread('imgs/opencv_logo.png', 0)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(131),\n",
    "plt.imshow(img_gray, cmap='gray')  # include cmap='gray' to display gray image\n",
    "plt.title('Gray'),plt.xticks([]), plt.yticks([])\n",
    "\n",
    "img_color1= cv2.imread('imgs/opencv_logo.png', 1)\n",
    "plt.subplot(132),plt.imshow(img_color1),\n",
    "plt.title('Color'),plt.xticks([]), plt.yticks([])\n",
    "\n",
    "img_color2= cv2.imread('imgs/opencv_logo.png',-1)\n",
    "plt.subplot(133),plt.imshow(img_color2),\n",
    "plt.title('Color'),plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How many channels for each image: img_gray, img_color1, img_color2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**: \n",
    "* img_gray:\n",
    "* img_color1:\n",
    "* img_color2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "## Scaling\n",
    "Resize image using the function [cv2.resize](https://docs.opencv.org/3.0-beta/modules/imgproc/doc/geometric_transformations.html?highlight=cv2.resize#cv2.resize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of available flags\n",
    "flags = [i for i in dir(cv2) if i.startswith('INTER_')]\n",
    "print (flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/opencv_logo1.png', 1)\n",
    "res = cv2.resize(img,None,fx=2.0, fy=2.0, interpolation = cv2.INTER_CUBIC)\n",
    "#OR\n",
    "height, width = img.shape[:2]\n",
    "res = cv2.resize(img,(2*width, 2*height), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "################################################################################\n",
    "# TO DO: Check the size of 'img' and 'res'?\n",
    "################################################################################\n",
    "pass\n",
    "print(img.shape)\n",
    "print(res.shape)\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "################################################################################\n",
    "# TO DO: Resize 'img' so as to the smaller side is 500, while keeping image \n",
    "# ration unchanged.\n",
    "x = (500/378)\n",
    "img2 = cv2.resize(img,(int(x*width),int(x*height)),interpolation =cv2.INTER_CUBIC)\n",
    "print(img2.shape)\n",
    "#plt.imshow(img)\n",
    "plt.imshow(img2)\n",
    "print(width/height,566/500)\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation\n",
    "Translation is the shifting of object's location. If you know the shift in $(x,y)$ direction, let it be $(t_x,t_y)$, you can create the transformation matrix $M$ as follows:\n",
    "\n",
    "$$M = \\begin{bmatrix} 1 & 0 & t_x \\\\ 0 & 1 & t_y \\end{bmatrix}$$\n",
    "You can take make it into a Numpy array of type **np.float32** and pass it into [cv2.warpAffine()](https://docs.opencv.org/3.0-beta/modules/imgproc/doc/geometric_transformations.html?highlight=cv2.warpaffine#cv2.warpAffine) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/opencv_logo1.png', 1)\n",
    "rows,cols,_ = img.shape\n",
    "M = np.float32([[1,0,100],[0,1,50]]) # Shift right by 100 and down by 50\n",
    "dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "\n",
    "################################################################################\n",
    "# TO DO: Observed that the bottom right of 'dst' image is lost. Modifying the\n",
    "# following codeline so as to the 'res' image is fully shown.\n",
    "################################################################################\n",
    "res = cv2.warpAffine(img,M,(cols,rows))\n",
    "N = np.float32([[1,0,-100],[0,1,-50]])\n",
    "res = cv2.warpAffine(res,N,(cols,rows))\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(131),plt.imshow(img),\n",
    "plt.title('Original'),plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(132),plt.imshow(dst),\n",
    "plt.title('Shifted images'),plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(133),plt.imshow(res),\n",
    "plt.title('Shifted images'),plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotation\n",
    "Calculates an affine matrix of 2D rotation using [cv2.getRotationMatrix2D()](https://docs.opencv.org/3.0-beta/modules/imgproc/doc/geometric_transformations.html?highlight=cv2.getrotationmatrix2d#cv2.getRotationMatrix2D).\n",
    " * 1st argument: center\n",
    " * 2nd argument: angle (in degree)\n",
    " * 3rd argument: scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/opencv_logo1.png', 1)\n",
    "H,W,_ = img.shape\n",
    "################################################################################\n",
    "# TO DO: Run the code to observe the output image.\n",
    "# Modifying the code below so as to the 'dst' image has no black padding.\n",
    "################################################################################\n",
    "M = cv2.getRotationMatrix2D((W/2,H/2),90,1)\n",
    "\n",
    "dst = cv2.warpAffine(img,M,(W-50,H))\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "plt.imshow(dst),\n",
    "plt.title('Rotated images'),plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing color space - Grayscale\n",
    "Grayscale values is converted from RGB values by a weighted sum of the R, G, and B components:\n",
    "\n",
    "$$0.2989 \\times R + 0.5870 \\times G + 0.1140 \\times B $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-19be65872062>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Split channels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'imgs/balls.jpg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m131\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# Split channels\n",
    "img = cv2.imread('imgs/balls.jpg', 1)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(131),plt.imshow(img[:,:,0], cmap='gray'),\n",
    "plt.title('Blue channel'),plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(132),plt.imshow(img[:,:,1], cmap='gray'),\n",
    "plt.title('Green channel'),plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(133),plt.imshow(img[:,:,2], cmap='gray'),\n",
    "plt.title('Red channel'),plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(img):\n",
    "    \"\"\"\n",
    "    A implementation of the method that converts BGR image to grayscale image of \n",
    "    uint8 data type.\n",
    "    \"\"\"\n",
    "    b, g, r = img[:,:,0], img[:,:,1], img[:,:,2]\n",
    "    out = np.round(0.2989 * r + 0.5870 * g + 0.1140 * b,0)\n",
    "    out = out.astype('int')\n",
    "\n",
    "    ################################################################################\n",
    "    # TO DO: Implement the method to convert BGR image to Grayscale image.         #\n",
    "    # Hint: Remember to round and convert the values to nearest uint8 values.      #\n",
    "    ################################################################################\n",
    "    pass\n",
    "    ################################################################################\n",
    "    #                                 END OF YOUR CODE                             #\n",
    "    ################################################################################\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/balls.jpg', 1)\n",
    "\n",
    "out = rgb2gray(img)\n",
    "plt.imshow(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code section to compare your implementation of the `rgb2gray` function with OpenCV built-in function [cv2.cvtColor](https://docs.opencv.org/3.0-beta/modules/imgproc/doc/miscellaneous_transformations.html?highlight=cv2.cvtcolor#cv2.cvtColor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/ball_red.jpg', 1)\n",
    "img_gray1 = rgb2gray(img)\n",
    "img_gray2 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(121),plt.imshow(img_gray1, cmap='gray'),\n",
    "plt.title('My rgb2gray'),plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(img_gray1 - img_gray2, cmap='gray'),\n",
    "plt.title('Difference'),plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Check your output: count \n",
    "print('Testing rgb2gray')\n",
    "print('Number of difference pixel is %d' % np.count_nonzero(img_gray1 - img_gray2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Does your implementation of `rgb2gray` function give the result that is exactly the same as OpenCV built-in function? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** fill in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing color space - Detect object by color. \n",
    "By converting BGR image to HSV, we can use this to extract a colored object. In HSV, it is more easier to represent a color than RGB color-space. In this exercise, we will try to extract blue, red, and yellow colored objects. So here is the method:\n",
    "\n",
    " * Take each frame of the video\n",
    " * Convert from BGR to HSV color-space \n",
    " * We threshold the HSV image for a range of blue color\n",
    " * Now extract the blue object alone, we can do whatever on that image we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of available flags\n",
    "flags = [i for i in dir(cv2) if i.startswith('COLOR_')]\n",
    "print(flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = cv2.imread('imgs/balls.jpg', 1)\n",
    "\n",
    "# Convert BGR to RGB, now you will see the color of 'frame' image\n",
    "# is displayed properly.\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert BGR to HSV\n",
    "hsv = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "# define range of blue color in HSV\n",
    "lower_blue = np.array([110,50,50])\n",
    "upper_blue = np.array([130,255,255])\n",
    "\n",
    "# Threshold the HSV image to get only blue colors\n",
    "mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "# Bitwise-AND mask and original image\n",
    "res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "\n",
    "################################################################################\n",
    "# TO DO: Implement masks for red and yellow balls. \n",
    "################################################################################\n",
    "red = np.uint8([[[0,0,255 ]]])\n",
    "hsv_red = cv2.cvtColor(red,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "print(hsv_red)\n",
    "lower_red = np.array([0,50,50])\n",
    "upper_red = np.array([10,255,255])\n",
    "mask_red = cv2.inRange(hsv,lower_red,upper_red)\n",
    "#mask_red = cv2.inRange(hsv_red,lower_red,upper_red)\n",
    "res_red = cv2.bitwise_and(frame,frame,mask=mask_red)\n",
    "\n",
    "\n",
    "#yellow\n",
    "yellow = np.uint8([[[50,225,235]]])\n",
    "hsv_yellow = cv2.cvtColor(yellow,cv2.COLOR_BGR2HSV)\n",
    "lower_yellow = np.array([18,50,50])\n",
    "upper_yellow = np.array([38,255,255])\n",
    "mask_yel = cv2.inRange(hsv,lower_yellow,upper_yellow)\n",
    "res_yellow = cv2.bitwise_and(frame,frame,mask=mask_yel)\n",
    "print(hsv_yellow)\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(331),plt.imshow(frame),\n",
    "plt.title('Original'),plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(332),plt.imshow(mask, cmap='gray'),\n",
    "plt.title('Mask'),plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(333),plt.imshow(res),\n",
    "plt.title('Output'),plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(334),plt.imshow(mask_red, cmap='gray'),\n",
    "plt.title('Red Mask'),plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(335),plt.imshow(res_red),\n",
    "plt.title('Red Output'),plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(336),plt.imshow(mask_yel, cmap='gray'),\n",
    "plt.title('Yellow Mask'),plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(337),plt.imshow(res_yellow),\n",
    "plt.title('Yellow Output'),plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Convolution ( Image Filtering )\n",
    "\n",
    "OpenCV provides a function, [cv2.filter2D](https://docs.opencv.org/3.0-beta/modules/imgproc/doc/filtering.html?highlight=cv2.filter2d#cv2.filter2D), to convolve a kernel with an image.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_naive(x, F, conv_param):\n",
    "    \"\"\"\n",
    "    A naive implementation of a convolutional filter.\n",
    "    \n",
    "    The input consists of a gray scale image x (1 channel) with height H and width\n",
    "    W. We convolve each input with filter F, which has height HH and width HH.\n",
    "    \n",
    "    Input:\n",
    "    - x: Input data of shape (H, W)\n",
    "    - F: Filter weights of shape (HH, WW)\n",
    "    - conv_param: A dictionary with the following keys:\n",
    "      - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "        horizontal and vertical directions.\n",
    "      - 'pad': The number of pixels that will be used to zero-pad the input.\n",
    "    \n",
    "    Return:\n",
    "    - out: Output data, of shape (H', W') where H' and W' are given by\n",
    "      H' = 1 + (H + 2 * pad - HH) / stride\n",
    "      W' = 1 + (W + 2 * pad - WW) / stride\n",
    "    \"\"\"\n",
    "    \n",
    "    stride = conv_param['stride']\n",
    "    pad = conv_param['pad']\n",
    "    H, W = x.shape\n",
    "    HH, WW = F.shape\n",
    "    H_prime = int(1 + (H + 2 * pad - HH) / stride)\n",
    "    W_prime = int(1 + (W + 2 * pad - WW) / stride)\n",
    "    x_pad = np.lib.pad(x, ((pad, pad), (pad, pad)),\\\n",
    "                            'constant', constant_values=(0))\n",
    "    out = np.zeros((H_prime, W_prime), dtype=x.dtype)\n",
    "    print(x_pad.shape)\n",
    "    #############################################################################\n",
    "    # TODO: Implement the convolutional forward pass.                           #\n",
    "    # Hint: Using 2 nested for-loop to calculate each pixel of the output image.#\n",
    "    #############################################################################\n",
    "    for y in range(pad, pad+H):\n",
    "        for x_coor in range(pad,pad+W):\n",
    "            roi = x_pad[y - pad:y + pad + 1, x_coor - pad:x_coor + pad + 1]\n",
    "            k = (roi * F).sum()\n",
    "            out[y - pad, x_coor - pad] = k\n",
    "    pass\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code section to test your implementation of the `convolution_naive` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shape = (5, 5)\n",
    "F_shape = (3, 3)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "F = np.linspace(-0.2, 0.3, num=np.prod(F_shape)).reshape(F_shape)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "out = convolution_naive(x, F, conv_param)\n",
    "correct_out = np.array( [[ 0.0075,     0.030625,   0.0521875,  0.07375,    0.0475   ],\n",
    "                         [ 0.114375,   0.1725,     0.18375,    0.195,      0.10875  ],\n",
    "                         [ 0.1753125,  0.22875,    0.24,       0.25125,    0.1228125],\n",
    "                         [ 0.23625,    0.285,      0.29625,    0.3075,     0.136875 ],\n",
    "                         [ 0.0075,    -0.05375,   -0.0603125, -0.066875,  -0.1025   ]])\n",
    "# print(correct_out.shape)\n",
    "# print(out)\n",
    "\n",
    "# Compare your output to ours; difference should be very small\n",
    "print('Testing convolution_naive')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of available BORDER effect\n",
    "flags = [i for i in dir(cv2) if i.startswith('BORDER_')]\n",
    "print(flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging filter\n",
    "This is done by convolving image with a normalized box filter. A $5\\times 5$ normalized box filter would look like below:\n",
    "$$K = \\frac{1}{25} \\begin{bmatrix} 1&1&1&1&1 \\\\ 1&1&1&1&1 \\\\ 1&1&1&1&1 \\\\ 1&1&1&1&1 \\\\1&1&1&1&1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert image data type from uint8 to float32.\n",
    "img = cv2.imread('imgs/text.png', 1).astype(np.float32)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "kernel = np.zeros((5,5), np.float32)\n",
    "################################################################################\n",
    "# TODO: Create a 5x5 kernel as K shown above.                                  #\n",
    "################################################################################\n",
    "K = np.ones((5,5),np.float32)/25\n",
    "pass\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "blur_2dfilter = cv2.filter2D(img,-1,K)\n",
    "\n",
    "# The above codes can be replaced by the following code line.\n",
    "blur = cv2.blur(img,(5,5))\n",
    "\n",
    "# Check your output; difference should be around 4e-3\n",
    "print('Testing convolution_naive')\n",
    "print('difference: ', rel_error(blur_2dfilter, blur))\n",
    "\n",
    "\n",
    "# Visualize the output image\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(121),plt.imshow(img, cmap='gray'),\n",
    "plt.title('Original'),plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(blur, cmap='gray'),\n",
    "plt.title('Average Blur'),plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Blurring\n",
    "Here is the 1D Gaussian distribution:\n",
    "$$G(x)=\\frac{1}{\\sigma\\sqrt{2\\pi} }\\exp{\\left(-\\frac{x^2}{\\sigma^2}\\right)}$$\n",
    "\n",
    "![1D Gaussian](imgs/Smoothing_Tutorial_theory_gaussian_0.jpg \"1D Gaussian\")\n",
    "\n",
    "Similarly, we have 2D Gaussian distribution.\n",
    "$$G(x,y)=\\frac{1}{2\\pi \\sigma^2}\\exp{\\left(-\\frac{x^2+y^2}{\\sigma^2}\\right)}$$\n",
    "The nearest neighboring pixels have the most influence.\n",
    "![2D Gaussian](imgs/gaussian_2d.gif \"2D Gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/text.png', 1).astype(np.float32)/255.0\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "gaussian_kernel_XY = np.zeros((5,5), np.float32)\n",
    "################################################################################\n",
    "# TODO: Create a 5x5 kernel, 'gaussian_kernel_XY', which approximates the \n",
    "# Gaussian function with sigma=1. \n",
    "# Hint: + You should NOT munually create the kernel.  \n",
    "#       + Use the 'cv2.getGaussianKernel' function to create 1D Guassian kernel.\n",
    "#       + Use the associative property of convolution to create 2D Gaussian. A\n",
    "# useful reference: https://blogs.mathworks.com/steve/2006/10/04/separable-convolution/\n",
    "################################################################################\n",
    "gauss_x = cv2.getGaussianKernel(5,1)\n",
    "gauss_y = np.transpose(gauss_x)\n",
    "gaussian_kernel_XY = np.dot(gauss_x,gauss_y)\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "blur_2dfilter = cv2.filter2D(img,-1,gaussian_kernel_XY)\n",
    "\n",
    "# The above codes can be replaced by the following code line.\n",
    "blur = cv2.GaussianBlur(img,(5,5),1)\n",
    "\n",
    "# Check your output; difference should be around 4e-3\n",
    "print('Testing convolution_naive')\n",
    "print('difference: ', rel_error(blur_2dfilter, blur))\n",
    "\n",
    "# Visualize the output image\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(121),plt.imshow(img, cmap='gray'),\n",
    "plt.title('Original'),plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(blur, cmap='gray'),\n",
    "plt.title('Guassian Blur'),plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Provide your comments on the outputs of *a average filter* and *a Gaussian filter*? Which one is more preferable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** fill in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median Filter\n",
    "\n",
    "Example: \n",
    " * **Odd** number of elements: $X=[2, 5, 1, 0, 9]\\to X_{sorted}=[0,1,2,5,9]\\Rightarrow \\text{median}=2$\n",
    " * **Even** number of elements: \n",
    "    * Option 1: $X=[5, 1, 0, 9]\\to X_{sorted}=[0,1,5,9]\\Rightarrow \\text{median}=1$\n",
    "    * Option 2: $X=[5, 1, 0, 9]\\to X_{sorted}=[0,1,5,9]\\Rightarrow \\text{median}=(1+5)/2=3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a function to find median value with `option 1`.\n",
    "import math\n",
    "def findMedian(x):\n",
    "    out = 0\n",
    "    #############################################################################\n",
    "    # TODO: Implement the function to find median value of array x.             #\n",
    "    # NOTE: You should see that the `median' numpy built-in function is based   #\n",
    "    # on option 2.\n",
    "    #############################################################################\n",
    "    flat = np.array(x).flatten()\n",
    "    sort = sorted(flat)\n",
    "    if len(sort)%2 == 0:\n",
    "        out = sort[(len(sort)//2)-1]\n",
    "    else:\n",
    "        out = sort[math.floor(len(sort)/2)]\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return float(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Numpy median: ', np.median([[5,1],[0,9]]))\n",
    "print ('Numpy median: ', np.median([2,5,1,0,9]))\n",
    "print ('findMedian: ', findMedian([[5,1],[0,9]]))\n",
    "print ('findMedian: ', findMedian([2,5,1,0,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/SaltAndPepperNoise.jpg', 0)\n",
    "median = cv2.medianBlur(img,5)\n",
    "gau_blur = cv2.GaussianBlur(img,(5,5),1)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(131),plt.imshow(img, 'gray')\n",
    "plt.title('Original'),plt.xticks([]),plt.yticks([])\n",
    "plt.subplot(132),plt.imshow(median, 'gray')\n",
    "plt.title('Median Blur'),plt.xticks([]),plt.yticks([])\n",
    "plt.subplot(133),plt.imshow(gau_blur, 'gray')\n",
    "plt.title('Gaussian Blur'),plt.xticks([]),plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Provide your comments on the effectiveness of *a median filter* and *a Gaussian filter* for the example above? Explain why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** fill in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myMedianBlur(img, size):\n",
    "    \"\"\"\n",
    "    A implementation of median blur filter.\n",
    "    \"\"\"\n",
    "    out = img.copy()\n",
    "    W,H = img.shape[0],img.shape[1]\n",
    "    s = (size - 1)/2\n",
    "    #############################################################################\n",
    "    # TODO: Implement the median blur.                                          #\n",
    "    # NOTE: Your implementation is NOT necessary to provide the identical       #\n",
    "    # output as OpenCV built-in function. However, it should be visually very   # \n",
    "    # similar.                                                                  #\n",
    "    #############################################################################\n",
    "    temp = []\n",
    "    img_final = np.zeros((W,H))\n",
    "    for i in range(W):\n",
    "        for j in range(H):\n",
    "            for k in range(size):\n",
    "                if i + k - size < 0 or i + k - size > W - 1: #checking for boundaries\n",
    "                    for l in range(size):\n",
    "                        temp.append(0)\n",
    "                else:\n",
    "                    if j + k - size < 0 or j + size > H - 1: #checking for boundaries\n",
    "                        temp.append(0)\n",
    "                    else:\n",
    "                        for l in range(size):\n",
    "                            temp.append(out[i + k - size][j + l - size])\n",
    "\n",
    "            temp.sort()\n",
    "            img_final[i][j] = temp[len(temp) // 2]\n",
    "            temp = []\n",
    "\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/SaltAndPepperNoise.jpg', 0)\n",
    "mymedian = myMedianBlur(img,5)\n",
    "median = cv2.medianBlur(img,5)\n",
    "\n",
    "# Note that your implementation is NOT necessary to provide \n",
    "# the identical output as OpenCV built-in function. However,\n",
    "# it should visually very similar.\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(121),plt.imshow(median, 'gray')\n",
    "plt.title('Opencv Median Blur'),plt.xticks([]),plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(median, 'gray')\n",
    "plt.title('My Median Blur'),plt.xticks([]),plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image gradient\n",
    "For 1-D continuous function $f(x)$, the gradient is given as:\n",
    "$$D_x[f(x)]=\\frac{d}{dx}f(x)=\\lim\\limits_{\\Delta x\\to 0}\\frac{f(x+\\Delta x)-f(x)}{\\Delta x},\\quad\\text{or}\\quad \\lim\\limits_{\\Delta x\\to 0}\\frac{f(x+\\Delta x)-f(x-\\Delta x)}{2\\Delta x}$$ \n",
    "\n",
    "For 1-D discrete function $f[n]$, the gradient becomes difference.\n",
    "$$D_n[f[n]]=f[n+1]-f[n],\\quad\\text{ or }\\quad\\frac{f[n+1]-f[n-1]}{2}$$\n",
    "\n",
    "The kernel to find gradient in 1-D discrete function is $[1, 0, -1]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/banded_vertical.jpg', 0).astype(np.float32) \n",
    "\n",
    "#############################################################################\n",
    "# TODO: Create a 3x3 kernel, Kx, to find the gradient in x-axis of an image.#\n",
    "#############################################################################\n",
    "kx = [[1,0,-1],[1,0,-1],[1,0,-1]]\n",
    "Kx = np.array(kx)\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "dstx = cv2.filter2D(img,-1, Kx)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121),plt.imshow(img, cmap='gray')\n",
    "plt.title('Original'),plt.xticks([]),plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(np.abs(dstx), cmap='gray')\n",
    "plt.title('Output 1'),plt.xticks([]),plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/banded_horizontal.jpg', 0).astype(np.float32)\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Create a 3x3 kernel, Ky, to find the gradient in y-axis of an image.#\n",
    "#############################################################################\n",
    "ky = [[1,1,1],[0,0,0],[-1,-1,-1]]\n",
    "Ky = np.array(ky)\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "dsty = cv2.filter2D(img,-1,Ky)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121),plt.imshow(img, 'gray')\n",
    "plt.title('Original'),plt.xticks([]),plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(np.abs(dsty), 'gray')\n",
    "plt.title('Output'),plt.xticks([]),plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What do the kernel `Kx` and `Ky` do in *image processing*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** fill in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two directions:\n",
    "* Find the difference: in the two directions: \n",
    "$$g_x[m,n]=f[m+1,n]-f[m-1,n]$$\n",
    "\n",
    "$$g_y[m,n]=f[m,n+1]-f[m,n-1]$$\n",
    "\n",
    "* Find the magnitude and direction of the gradient vector: \n",
    "$$\\|g[m,n]\\|=\\sqrt{g^2_x[m,n]+g^2_y[m,n]}$$\n",
    "\n",
    "$$\\measuredangle g[m,n]=\\tan^{-1}\\left(\\frac{g_y[m,n]}{g_x[m,n]}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/chequered.jpg', 0).astype(np.float32)\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Using the theory provided above, compute the magnitude of 2         # \n",
    "# direction image gradient.                                                 #\n",
    "#############################################################################\n",
    "dst1 = np.sqrt(np.add(np.square(dstx),np.square(dsty)))\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "\n",
    "# You can achieve a similar (NOT identical) output with the following code line.\n",
    "K = np.array([[0, 1,0],\n",
    "              [1,-4,1],\n",
    "              [0, 1,0]], dtype=np.float32)\n",
    "dst2 = cv2.filter2D(img,-1,K)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(131),plt.imshow(img, 'gray')\n",
    "plt.title('Original'),plt.xticks([]),plt.yticks([])\n",
    "plt.subplot(132),plt.imshow(np.abs(dst1), 'gray')\n",
    "plt.title('Output 1'),plt.xticks([]),plt.yticks([])\n",
    "plt.subplot(133),plt.imshow(np.abs(dst2), 'gray')\n",
    "plt.title('Output 2'),plt.xticks([]),plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram \n",
    " * It is a graphical representation of the intensity distribution of an image.\n",
    " * It quantifies the number of pixels for each intensity value considered.\n",
    "\n",
    "## Histogram equilization\n",
    " * Equalization implies mapping one distribution (the given histogram) to another distribution (a wider and more uniform distribution of intensity values) so the intensity values are spreaded over the whole range.\n",
    " * To accomplish the equalization effect, the remapping should be the cumulative distribution function (cdf) (more details, refer to Learning OpenCV). For the histogram H(i), its cumulative distribution $H^{'}(i)$ is:\n",
    "\n",
    "$$H^{'}(i) = \\sum_{0 \\le j < i} H(j)$$\n",
    "\n",
    " * To use this as a remapping function, we have to normalize $H^{'}(i)$ such that the maximum value is 255 ( or the maximum value for the intensity of the image ). From the example above, the cumulative function is:\n",
    "\n",
    "![cumulative distribution function](imgs/Histogram_Equalization_Theory_2.jpg \"Cumulative Distribution Function (cdf)\")\n",
    "\n",
    "* Finally, we use a simple remapping procedure to obtain the intensity values of the equalized image:\n",
    "\n",
    "$$equalized( x, y ) = H^{'}( src(x,y) )$$\n",
    "\n",
    "\n",
    "![Histogram Equalization](imgs/histEqualize.png \"Histogram Equalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/sudoku-original.jpg',0)\n",
    "W,H = img.shape\n",
    "img_eq = cv2.equalizeHist(img)\n",
    "\n",
    "hist = np.histogram(img, bins=256, range=(0.0, 255.0))\n",
    "hist_eq = np.histogram(img_eq, bins=256, range=(0.0, 255.0))\n",
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.subplot(321),plt.imshow(img, cmap='gray'),plt.title('Original Image'),plt.xticks([]),plt.yticks([]) \n",
    "plt.subplot(322),plt.imshow(img_eq, cmap='gray'),plt.title('Equalized Image'),plt.xticks([]),plt.yticks([]) \n",
    "plt.subplot(323),plt.hist(img.ravel(), bins=256, range=(0.0, 255.0)),plt.title('Original Image: Histogram')\n",
    "plt.subplot(324),plt.hist(img_eq.ravel(), bins=256, range=(0.0, 255.0)),plt.title('Equalized Image: Histogram')\n",
    "plt.subplot(325),plt.plot(range(0,256),np.cumsum(hist[0])*255/(W*H)),plt.title('Original Image: normalized CDF')\n",
    "plt.subplot(326),plt.plot(range(0,256),np.cumsum(hist_eq[0])*255/(W*H)),plt.title('Equalized Image: normalized CDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUIZ:** Is histogram equalization reversible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** fill in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myEqualizeHist(img):\n",
    "    \"\"\"\n",
    "    A implementation of a histogram equalization for image of `uint8` data type.\n",
    "    \"\"\"\n",
    "    out = img\n",
    "    #############################################################################\n",
    "    # TODO: Implement the histogram equalization function.                      #\n",
    "    #############################################################################\n",
    "    out = np.array(out).flatten()\n",
    "    hist = np.zeros(256)\n",
    "\n",
    "    for i in out:\n",
    "        hist[i] += 1\n",
    "    sum_ = np.cumsum(hist)\n",
    "    \n",
    "    sum_ = ((sum_ - sum_.min()) * 255) / (sum_.max()-sum_.min())\n",
    "    out = sum_[out]\n",
    "    out = np.reshape(out,img.shape)\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the correctness of your implementation by plotting the \n",
    "# normalized CDF of equalized image\n",
    "img = cv2.imread('imgs/sudoku-original.jpg',0)\n",
    "W,H = img.shape\n",
    "img_myeq = myEqualizeHist(img)\n",
    "\n",
    "# Your implementation may NOT need to return an image that is \n",
    "# exactly the same as the one OpenCV build-in function does. \n",
    "# However, the normalized CDF should make sense.\n",
    "hist_myeq = np.histogram(img_myeq, bins=256, range=(0.0, 255.0))\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(range(0,256),np.cumsum(hist_myeq[0])*255/(W*H))\n",
    "plt.title('Equalized Image: normalized CDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold\n",
    "## Simple Threshold\n",
    "If pixel value is greater than a threshold value, it is assigned one value (may be white), else it is assigned another value (may be black). The function used is [cv2.threshold](https://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html?highlight=cv2.threshold#cv2.threshold). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-dc5dab99e663>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get list of available flags for thresholding styles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mflags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'THRESH_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# Get list of available flags for thresholding styles\n",
    "flags = [i for i in dir(cv2) if i.startswith('THRESH_')]\n",
    "print(flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Method\n",
    "\n",
    "It decides how thresholding value is calculated. The function used is [cv2.adaptiveThreshold](https://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html?highlight=cv2.adaptivethreshold#cv2.adaptiveThreshold).\n",
    "\n",
    " * cv2.ADAPTIVE_THRESH_MEAN_C : threshold value is the mean of neighbourhood area.\n",
    " * cv2.ADAPTIVE_THRESH_GAUSSIAN_C : threshold value is the weighted sum of neighbourhood values where weights are a gaussian window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/sudoku-original.jpg',0)\n",
    "img = cv2.medianBlur(img,5)\n",
    "img_mean = np.mean(img)\n",
    "\n",
    "C = 2\n",
    "ret,th1 = cv2.threshold(img,img_mean,255,cv2.THRESH_BINARY)\n",
    "th2 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C,\\\n",
    "            cv2.THRESH_BINARY,11,C)\n",
    "\n",
    "th3 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "            cv2.THRESH_BINARY,11,C)\n",
    "\n",
    "#############################################################################\n",
    "# TODO:                                                             #\n",
    "# Trying several value of constant C and observing how the output           #\n",
    "# thresholded images change.                                             #\n",
    "#############################################################################\n",
    "th4 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C,\\\n",
    "            cv2.THRESH_BINARY,11,3)\n",
    "th5 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C,\\\n",
    "            cv2.THRESH_BINARY,11,3)\n",
    "th6 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C,\\\n",
    "            cv2.THRESH_BINARY,11,4)\n",
    "th7 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C,\\\n",
    "            cv2.THRESH_BINARY,11,4)\n",
    "\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "\n",
    "titles = ['Original Image', 'Global Thresholding (v = {:.2f})'.format(img_mean),\n",
    "        'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding','AMT3','AGT3','AMT4','AGT4']\n",
    "images = [img, th1, th2, th3,th4,th5,th6,th7]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(8):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(images[i],'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
